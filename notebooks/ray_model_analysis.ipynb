{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Analysis: CPU vs GPU Model Training Results\n",
    "\n",
    "This notebook analyzes the combined results from:\n",
    "- **CPU Cluster**: 90 traditional ML models\n",
    "- **GPU Cluster**: 10 PyTorch deep learning models\n",
    "\n",
    "Results are loaded from the shared Delta table: `ryuta.ray.model_training_results`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Results from Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESULTS_TABLE = 'ryuta.ray.model_training_results'\n",
    "\n",
    "print(f\"Loading results from {RESULTS_TABLE}...\")\n",
    "\n",
    "# Load from Delta table\n",
    "results_spark_df = spark.table(RESULTS_TABLE)\n",
    "\n",
    "# Convert to pandas\n",
    "results_df = results_spark_df.toPandas()\n",
    "\n",
    "print(f\"Loaded {len(results_df)} model results\")\n",
    "print(f\"\\nColumns: {list(results_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary\n",
    "print(\"Dataset Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total models: {len(results_df)}\")\n",
    "print(f\"CPU models: {len(results_df[results_df['cluster_type'] == 'cpu'])}\")\n",
    "print(f\"GPU models: {len(results_df[results_df['cluster_type'] == 'gpu'])}\")\n",
    "print(f\"\\nModel types:\")\n",
    "print(results_df['model_type'].value_counts())\n",
    "print(f\"\\nFeature strategies:\")\n",
    "print(results_df['feature_strategy'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = ['accuracy', 'roc_auc', 'f1', 'precision', 'recall']\n",
    "\n",
    "print(\"\\nAll Models:\")\n",
    "for metric in metrics:\n",
    "    print(f\"  {metric.upper():12s}: Mean={results_df[metric].mean():.4f}, \"\n",
    "          f\"Std={results_df[metric].std():.4f}, \"\n",
    "          f\"Max={results_df[metric].max():.4f}, \"\n",
    "          f\"Min={results_df[metric].min():.4f}\")\n",
    "\n",
    "print(f\"\\nTraining Time:\")\n",
    "print(f\"  Total: {results_df['training_time'].sum():.2f}s ({results_df['training_time'].sum()/60:.2f} minutes)\")\n",
    "print(f\"  Mean: {results_df['training_time'].mean():.2f}s\")\n",
    "print(f\"  Median: {results_df['training_time'].median():.2f}s\")\n",
    "print(f\"  Max: {results_df['training_time'].max():.2f}s\")\n",
    "print(f\"  Min: {results_df['training_time'].min():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 MODELS BY ROC AUC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_10 = results_df.nlargest(10, 'roc_auc')\n",
    "\n",
    "display_cols = ['model_id', 'model_type', 'cluster_type', 'n_features_used', \n",
    "                'feature_strategy', 'roc_auc', 'accuracy', 'f1', 'training_time']\n",
    "\n",
    "print(top_10[display_cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 MODELS BY ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_10_acc = results_df.nlargest(10, 'accuracy')\n",
    "print(top_10_acc[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance by Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY MODEL TYPE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_type_stats = results_df.groupby('model_type').agg({\n",
    "    'roc_auc': ['mean', 'std', 'min', 'max'],\n",
    "    'accuracy': ['mean', 'std', 'min', 'max'],\n",
    "    'f1': ['mean', 'std', 'min', 'max'],\n",
    "    'training_time': ['mean', 'median', 'sum'],\n",
    "    'model_id': 'count'\n",
    "}).round(4)\n",
    "\n",
    "model_type_stats.columns = ['_'.join(col).strip() for col in model_type_stats.columns.values]\n",
    "model_type_stats = model_type_stats.rename(columns={'model_id_count': 'count'})\n",
    "\n",
    "# Sort by mean ROC AUC\n",
    "model_type_stats = model_type_stats.sort_values('roc_auc_mean', ascending=False)\n",
    "\n",
    "print(model_type_stats)\n",
    "\n",
    "# Find best model type\n",
    "best_model_type = model_type_stats['roc_auc_mean'].idxmax()\n",
    "print(f\"\\nBest performing model type (by mean ROC AUC): {best_model_type}\")\n",
    "print(f\"Mean ROC AUC: {model_type_stats.loc[best_model_type, 'roc_auc_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CPU vs GPU Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CPU vs GPU CLUSTER COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "cluster_stats = results_df.groupby('cluster_type').agg({\n",
    "    'roc_auc': ['mean', 'std', 'max'],\n",
    "    'accuracy': ['mean', 'std', 'max'],\n",
    "    'f1': ['mean', 'std', 'max'],\n",
    "    'training_time': ['mean', 'median', 'sum'],\n",
    "    'model_id': 'count'\n",
    "}).round(4)\n",
    "\n",
    "cluster_stats.columns = ['_'.join(col).strip() for col in cluster_stats.columns.values]\n",
    "cluster_stats = cluster_stats.rename(columns={'model_id_count': 'count'})\n",
    "\n",
    "print(cluster_stats)\n",
    "\n",
    "# Statistical comparison\n",
    "cpu_models = results_df[results_df['cluster_type'] == 'cpu']\n",
    "gpu_models = results_df[results_df['cluster_type'] == 'gpu']\n",
    "\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(f\"\\nCPU Models ({len(cpu_models)}):\")\n",
    "print(f\"  Mean ROC AUC: {cpu_models['roc_auc'].mean():.4f} ± {cpu_models['roc_auc'].std():.4f}\")\n",
    "print(f\"  Best ROC AUC: {cpu_models['roc_auc'].max():.4f}\")\n",
    "print(f\"  Total training time: {cpu_models['training_time'].sum():.2f}s ({cpu_models['training_time'].sum()/60:.2f} min)\")\n",
    "\n",
    "print(f\"\\nGPU Models ({len(gpu_models)}):\")\n",
    "print(f\"  Mean ROC AUC: {gpu_models['roc_auc'].mean():.4f} ± {gpu_models['roc_auc'].std():.4f}\")\n",
    "print(f\"  Best ROC AUC: {gpu_models['roc_auc'].max():.4f}\")\n",
    "print(f\"  Total training time: {gpu_models['training_time'].sum():.2f}s ({gpu_models['training_time'].sum()/60:.2f} min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance by Feature Selection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY FEATURE SELECTION STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "strategy_stats = results_df.groupby('feature_strategy').agg({\n",
    "    'roc_auc': ['mean', 'std', 'max'],\n",
    "    'accuracy': ['mean', 'std', 'max'],\n",
    "    'n_features_used': 'mean',\n",
    "    'training_time': 'mean',\n",
    "    'model_id': 'count'\n",
    "}).round(4)\n",
    "\n",
    "strategy_stats.columns = ['_'.join(col).strip() for col in strategy_stats.columns.values]\n",
    "strategy_stats = strategy_stats.rename(columns={'model_id_count': 'count'})\n",
    "strategy_stats = strategy_stats.sort_values('roc_auc_mean', ascending=False)\n",
    "\n",
    "print(strategy_stats)\n",
    "\n",
    "best_strategy = strategy_stats['roc_auc_mean'].idxmax()\n",
    "print(f\"\\nBest feature selection strategy: {best_strategy}\")\n",
    "print(f\"Mean ROC AUC: {strategy_stats.loc[best_strategy, 'roc_auc_mean']:.4f}\")\n",
    "print(f\"Mean features used: {strategy_stats.loc[best_strategy, 'n_features_used_mean']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Count vs Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE COUNT vs PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create bins for feature counts\n",
    "results_df['feature_bin'] = pd.cut(results_df['n_features_used'], \n",
    "                                     bins=[0, 25, 50, 75, 100], \n",
    "                                     labels=['1-25', '26-50', '51-75', '76-100'])\n",
    "\n",
    "feature_bin_stats = results_df.groupby('feature_bin').agg({\n",
    "    'roc_auc': ['mean', 'std', 'count'],\n",
    "    'accuracy': 'mean',\n",
    "    'n_features_used': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "feature_bin_stats.columns = ['_'.join(col).strip() for col in feature_bin_stats.columns.values]\n",
    "print(feature_bin_stats)\n",
    "\n",
    "# Correlation analysis\n",
    "correlation = results_df[['n_features_used', 'roc_auc']].corr().iloc[0, 1]\n",
    "print(f\"\\nCorrelation between number of features and ROC AUC: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING TIME ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training time by model type\n",
    "time_by_type = results_df.groupby('model_type')['training_time'].agg([\n",
    "    'count', 'mean', 'median', 'min', 'max', 'sum'\n",
    "]).round(2)\n",
    "\n",
    "time_by_type = time_by_type.sort_values('mean', ascending=False)\n",
    "print(\"\\nTraining Time by Model Type:\")\n",
    "print(time_by_type)\n",
    "\n",
    "# Fastest and slowest models\n",
    "fastest = results_df.nsmallest(5, 'training_time')[['model_id', 'model_type', 'cluster_type', 'training_time', 'roc_auc']]\n",
    "slowest = results_df.nlargest(5, 'training_time')[['model_id', 'model_type', 'cluster_type', 'training_time', 'roc_auc']]\n",
    "\n",
    "print(\"\\nFastest 5 Models:\")\n",
    "print(fastest.to_string(index=False))\n",
    "\n",
    "print(\"\\nSlowest 5 Models:\")\n",
    "print(slowest.to_string(index=False))\n",
    "\n",
    "# Efficiency metric: ROC AUC per second\n",
    "results_df['efficiency'] = results_df['roc_auc'] / results_df['training_time']\n",
    "most_efficient = results_df.nlargest(5, 'efficiency')[['model_id', 'model_type', 'roc_auc', 'training_time', 'efficiency']]\n",
    "\n",
    "print(\"\\nMost Efficient Models (ROC AUC / Training Time):\")\n",
    "print(most_efficient.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC distribution by model type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: ROC AUC by model type\n",
    "model_order = results_df.groupby('model_type')['roc_auc'].mean().sort_values(ascending=False).index\n",
    "sns.boxplot(data=results_df, y='model_type', x='roc_auc', order=model_order, ax=axes[0])\n",
    "axes[0].set_title('ROC AUC Distribution by Model Type', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('ROC AUC', fontsize=12)\n",
    "axes[0].set_ylabel('Model Type', fontsize=12)\n",
    "\n",
    "# Plot 2: ROC AUC by cluster type\n",
    "sns.boxplot(data=results_df, x='cluster_type', y='roc_auc', ax=axes[1])\n",
    "axes[1].set_title('ROC AUC Distribution: CPU vs GPU', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Cluster Type', fontsize=12)\n",
    "axes[1].set_ylabel('ROC AUC', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature count vs Performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Scatter plot\n",
    "for cluster in results_df['cluster_type'].unique():\n",
    "    cluster_data = results_df[results_df['cluster_type'] == cluster]\n",
    "    axes[0].scatter(cluster_data['n_features_used'], cluster_data['roc_auc'], \n",
    "                   label=cluster.upper(), alpha=0.6, s=50)\n",
    "\n",
    "axes[0].set_xlabel('Number of Features Used', fontsize=12)\n",
    "axes[0].set_ylabel('ROC AUC', fontsize=12)\n",
    "axes[0].set_title('Feature Count vs ROC AUC', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Training time by model type\n",
    "time_data = results_df.groupby('model_type')['training_time'].mean().sort_values()\n",
    "time_data.plot(kind='barh', ax=axes[1], color='steelblue')\n",
    "axes[1].set_xlabel('Mean Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_ylabel('Model Type', fontsize=12)\n",
    "axes[1].set_title('Average Training Time by Model Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by feature selection strategy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: ROC AUC by feature strategy\n",
    "strategy_order = results_df.groupby('feature_strategy')['roc_auc'].mean().sort_values(ascending=False).index\n",
    "sns.boxplot(data=results_df, y='feature_strategy', x='roc_auc', order=strategy_order, ax=axes[0])\n",
    "axes[0].set_title('ROC AUC by Feature Selection Strategy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('ROC AUC', fontsize=12)\n",
    "axes[0].set_ylabel('Feature Strategy', fontsize=12)\n",
    "\n",
    "# Plot 2: Model count by type and cluster\n",
    "model_counts = results_df.groupby(['cluster_type', 'model_type']).size().unstack(fill_value=0)\n",
    "model_counts.plot(kind='bar', stacked=True, ax=axes[1], colormap='tab10')\n",
    "axes[1].set_title('Model Distribution by Cluster and Type', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Cluster Type', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Models', fontsize=12)\n",
    "axes[1].legend(title='Model Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Model Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = results_df.loc[results_df['roc_auc'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest Overall Model:\")\n",
    "print(f\"  Model ID: {best_model['model_id']}\")\n",
    "print(f\"  Model Type: {best_model['model_type']}\")\n",
    "print(f\"  Cluster Type: {best_model['cluster_type']}\")\n",
    "print(f\"  Features Used: {best_model['n_features_used']}\")\n",
    "print(f\"  Feature Strategy: {best_model['feature_strategy']}\")\n",
    "print(f\"\\n  Performance Metrics:\")\n",
    "print(f\"    ROC AUC: {best_model['roc_auc']:.4f}\")\n",
    "print(f\"    Accuracy: {best_model['accuracy']:.4f}\")\n",
    "print(f\"    F1 Score: {best_model['f1']:.4f}\")\n",
    "print(f\"    Precision: {best_model['precision']:.4f}\")\n",
    "print(f\"    Recall: {best_model['recall']:.4f}\")\n",
    "print(f\"\\n  Training Details:\")\n",
    "print(f\"    Training Time: {best_model['training_time']:.2f} seconds\")\n",
    "print(f\"    Device: {best_model.get('device', 'N/A')}\")\n",
    "\n",
    "if 'best_hyperparams' in best_model and pd.notna(best_model['best_hyperparams']):\n",
    "    try:\n",
    "        hyperparams = json.loads(best_model['best_hyperparams'])\n",
    "        print(f\"\\n  Best Hyperparameters:\")\n",
    "        for param, value in hyperparams.items():\n",
    "            print(f\"    {param}: {value}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model from each cluster\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODELS FROM EACH CLUSTER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster in ['cpu', 'gpu']:\n",
    "    cluster_df = results_df[results_df['cluster_type'] == cluster]\n",
    "    if len(cluster_df) > 0:\n",
    "        best_in_cluster = cluster_df.loc[cluster_df['roc_auc'].idxmax()]\n",
    "        \n",
    "        print(f\"\\nBest {cluster.upper()} Model:\")\n",
    "        print(f\"  Model ID: {best_in_cluster['model_id']}\")\n",
    "        print(f\"  Model Type: {best_in_cluster['model_type']}\")\n",
    "        print(f\"  ROC AUC: {best_in_cluster['roc_auc']:.4f}\")\n",
    "        print(f\"  Accuracy: {best_in_cluster['accuracy']:.4f}\")\n",
    "        print(f\"  Features: {best_in_cluster['n_features_used']} ({best_in_cluster['feature_strategy']})\")\n",
    "        print(f\"  Training Time: {best_in_cluster['training_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Best model type\n",
    "best_type = results_df.groupby('model_type')['roc_auc'].mean().idxmax()\n",
    "best_type_score = results_df.groupby('model_type')['roc_auc'].mean().max()\n",
    "\n",
    "print(f\"\\n1. Best Model Family:\")\n",
    "print(f\"   {best_type} achieved the highest average ROC AUC of {best_type_score:.4f}\")\n",
    "\n",
    "# Best feature strategy\n",
    "best_strat = results_df.groupby('feature_strategy')['roc_auc'].mean().idxmax()\n",
    "best_strat_score = results_df.groupby('feature_strategy')['roc_auc'].mean().max()\n",
    "\n",
    "print(f\"\\n2. Best Feature Selection Strategy:\")\n",
    "print(f\"   '{best_strat}' strategy achieved the highest average ROC AUC of {best_strat_score:.4f}\")\n",
    "\n",
    "# CPU vs GPU comparison\n",
    "cpu_avg = results_df[results_df['cluster_type'] == 'cpu']['roc_auc'].mean()\n",
    "gpu_avg = results_df[results_df['cluster_type'] == 'gpu']['roc_auc'].mean()\n",
    "\n",
    "print(f\"\\n3. CPU vs GPU Performance:\")\n",
    "if cpu_avg > gpu_avg:\n",
    "    print(f\"   CPU models performed better on average (CPU: {cpu_avg:.4f} vs GPU: {gpu_avg:.4f})\")\n",
    "    print(f\"   Traditional ML models may be more suitable for this dataset\")\n",
    "else:\n",
    "    print(f\"   GPU models performed better on average (GPU: {gpu_avg:.4f} vs CPU: {cpu_avg:.4f})\")\n",
    "    print(f\"   Deep learning models may capture more complex patterns\")\n",
    "\n",
    "# Feature count insight\n",
    "corr = results_df[['n_features_used', 'roc_auc']].corr().iloc[0, 1]\n",
    "print(f\"\\n4. Feature Count Impact:\")\n",
    "if abs(corr) < 0.1:\n",
    "    print(f\"   Weak correlation ({corr:.3f}) between feature count and performance\")\n",
    "    print(f\"   Feature quality matters more than quantity\")\n",
    "elif corr > 0:\n",
    "    print(f\"   Positive correlation ({corr:.3f}) - more features generally help\")\n",
    "else:\n",
    "    print(f\"   Negative correlation ({corr:.3f}) - fewer features may reduce overfitting\")\n",
    "\n",
    "# Training efficiency\n",
    "fastest_type = results_df.groupby('model_type')['training_time'].mean().idxmin()\n",
    "fastest_time = results_df.groupby('model_type')['training_time'].mean().min()\n",
    "\n",
    "print(f\"\\n5. Training Efficiency:\")\n",
    "print(f\"   {fastest_type} was the fastest to train (avg: {fastest_time:.2f}s)\")\n",
    "\n",
    "# Overall recommendation\n",
    "print(f\"\\n6. Overall Recommendation:\")\n",
    "print(f\"   For best performance: Use {best_type} with '{best_strat}' feature selection\")\n",
    "print(f\"   Expected ROC AUC: ~{results_df[(results_df['model_type'] == best_type) & (results_df['feature_strategy'] == best_strat)]['roc_auc'].mean():.4f}\")\n",
    "\n",
    "# Parallel execution benefit\n",
    "total_time = results_df['training_time'].sum()\n",
    "print(f\"\\n7. Parallel Execution Benefit:\")\n",
    "print(f\"   Total training time for all models: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "print(f\"   With Ray parallel execution, this was achieved much faster than sequential training\")\n",
    "print(f\"   Estimated sequential time: ~{total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "print(f\"   Actual wall-clock time depends on cluster parallelization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    'total_models': len(results_df),\n",
    "    'cpu_models': len(results_df[results_df['cluster_type'] == 'cpu']),\n",
    "    'gpu_models': len(results_df[results_df['cluster_type'] == 'gpu']),\n",
    "    'best_overall_model_id': int(results_df['roc_auc'].idxmax()),\n",
    "    'best_roc_auc': float(results_df['roc_auc'].max()),\n",
    "    'mean_roc_auc': float(results_df['roc_auc'].mean()),\n",
    "    'best_model_type': best_type,\n",
    "    'best_feature_strategy': best_strat,\n",
    "    'total_training_time_seconds': float(results_df['training_time'].sum()),\n",
    "    'mean_training_time_seconds': float(results_df['training_time'].mean())\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(json.dumps(summary_report, indent=2))\n",
    "\n",
    "# Save summary to Delta table\n",
    "summary_df = pd.DataFrame([summary_report])\n",
    "summary_df['analysis_timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "summary_spark_df = spark.createDataFrame(summary_df)\n",
    "\n",
    "summary_table = 'ryuta.ray.model_training_summary'\n",
    "print(f\"\\nSaving summary report to {summary_table}...\")\n",
    "\n",
    "summary_spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(summary_table)\n",
    "\n",
    "print(\"Summary report saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive analysis of all 100 models trained across CPU and GPU clusters:\n",
    "\n",
    "### Analyses Performed:\n",
    "1. Overall performance metrics across all models\n",
    "2. Top 10 best performing models identification\n",
    "3. Performance comparison by model type\n",
    "4. CPU vs GPU cluster comparison\n",
    "5. Feature selection strategy analysis\n",
    "6. Feature count vs performance relationship\n",
    "7. Training time and efficiency analysis\n",
    "8. Visual comparisons and distributions\n",
    "9. Best model detailed breakdown\n",
    "10. Key insights and recommendations\n",
    "\n",
    "### Key Takeaways:\n",
    "- Identified the best performing model families\n",
    "- Determined optimal feature selection strategies\n",
    "- Compared CPU vs GPU training effectiveness\n",
    "- Analyzed the impact of feature count on performance\n",
    "- Evaluated training efficiency across different models\n",
    "\n",
    "Use these insights to:\n",
    "- Select the best model for production deployment\n",
    "- Optimize feature engineering pipelines\n",
    "- Choose appropriate compute resources for future training\n",
    "- Make data-driven decisions about model selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
