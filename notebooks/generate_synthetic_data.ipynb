{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Classification Dataset\n",
    "\n",
    "This notebook generates a synthetic dataset with:\n",
    "- 100 features\n",
    "- 1 binary label (0 or 1)\n",
    "- Suitable for machine learning classification tasks\n",
    "\n",
    "The dataset is written to a Databricks Delta table: `ryuta.ray.synthetic_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "N_SAMPLES = 10000  # Number of samples\n",
    "N_FEATURES = 100   # Number of features\n",
    "N_INFORMATIVE = 50 # Number of informative features\n",
    "N_REDUNDANT = 25   # Number of redundant features\n",
    "N_CLASSES = 2      # Binary classification\n",
    "CLASS_BALANCE = [0.6, 0.4]  # Slight class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification dataset\n",
    "print(f\"Generating synthetic dataset with {N_SAMPLES} samples and {N_FEATURES} features...\")\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=N_SAMPLES,\n",
    "    n_features=N_FEATURES,\n",
    "    n_informative=N_INFORMATIVE,\n",
    "    n_redundant=N_REDUNDANT,\n",
    "    n_classes=N_CLASSES,\n",
    "    weights=CLASS_BALANCE,\n",
    "    flip_y=0.01,  # Add 1% label noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset generated successfully!\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label vector shape: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame\n",
    "# Feature columns named as feature_0, feature_1, ..., feature_99\n",
    "feature_columns = [f'feature_{i}' for i in range(N_FEATURES)]\n",
    "df = pd.DataFrame(X, columns=feature_columns)\n",
    "\n",
    "# Add the binary label column\n",
    "df['label'] = y\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"Summary statistics for first 5 features and label:\")\n",
    "df[['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'label']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "print(\"Converting to Spark DataFrame...\")\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Show schema\n",
    "print(\"\\nSpark DataFrame Schema:\")\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst few rows in Spark DataFrame:\")\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Delta table name\n",
    "TABLE_NAME = \"ryuta.ray.synthetic_data\"\n",
    "\n",
    "print(f\"Writing dataset to Delta table: {TABLE_NAME}...\")\n",
    "\n",
    "# Write to Delta table (overwrite mode)\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(TABLE_NAME)\n",
    "\n",
    "print(f\"\\nDataset successfully written to {TABLE_NAME}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the table was created and data was written\n",
    "print(f\"Verifying table {TABLE_NAME}...\")\n",
    "\n",
    "# Read from the Delta table\n",
    "verify_df = spark.table(TABLE_NAME)\n",
    "\n",
    "print(f\"\\nTable row count: {verify_df.count()}\")\n",
    "print(f\"\\nTable schema:\")\n",
    "verify_df.printSchema()\n",
    "\n",
    "print(f\"\\nSample data from table:\")\n",
    "verify_df.show(5)\n",
    "\n",
    "# Show label distribution in the table\n",
    "print(f\"\\nLabel distribution in table:\")\n",
    "verify_df.groupBy('label').count().orderBy('label').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display table details\n",
    "print(f\"Table details for {TABLE_NAME}:\")\n",
    "spark.sql(f\"DESCRIBE DETAIL {TABLE_NAME}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. Generated a synthetic dataset with 100 features and 1 binary label\n",
    "2. Created 10,000 samples suitable for classification tasks\n",
    "3. Written the data to the Delta table: `ryuta.ray.synthetic_data`\n",
    "\n",
    "The dataset includes:\n",
    "- 50 informative features that are useful for classification\n",
    "- 25 redundant features (linear combinations of informative features)\n",
    "- 25 random noise features\n",
    "- Slight class imbalance (60/40 split)\n",
    "- 1% label noise to make it more realistic\n",
    "\n",
    "You can now use this table for machine learning classification tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
