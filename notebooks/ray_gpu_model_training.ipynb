{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Cluster: Parallel Training of PyTorch Deep Learning Models with Ray\n",
    "\n",
    "This notebook trains **10 PyTorch MLP models** in parallel on a GPU cluster:\n",
    "- Various neural network architectures\n",
    "- Different feature subsets\n",
    "- Bayesian hyperparameter optimization\n",
    "\n",
    "**Cluster Configuration**: Single-node with 4 GPUs\n",
    "\n",
    "**Note**: Run this notebook on the GPU cluster. Results are saved to `ryuta.ray.model_training_results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Ray imports\n",
    "import ray\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "# Optuna for Bayesian optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Suppress Optuna logging\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Notebook type: GPU CLUSTER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'table_name': 'ryuta.ray.synthetic_data',\n",
    "    'results_table': 'ryuta.ray.model_training_results',\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'n_trials_per_model': 20,  # Bayesian optimization trials\n",
    "    \n",
    "    # Cluster configuration\n",
    "    'cluster_type': 'gpu',\n",
    "    'n_gpus': 4,\n",
    "    \n",
    "    # Model distribution (GPU models only)\n",
    "    'model_distribution': {\n",
    "        'pytorch_mlp': 10\n",
    "    },\n",
    "    \n",
    "    # Model ID offset (GPU models: 90-99)\n",
    "    'model_id_start': 90\n",
    "}\n",
    "\n",
    "CONFIG['n_models_total'] = sum(CONFIG['model_distribution'].values())\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Delta table\n",
    "print(f\"Loading data from {CONFIG['table_name']}...\")\n",
    "df_spark = spark.table(CONFIG['table_name'])\n",
    "\n",
    "print(f\"Total rows: {df_spark.count()}\")\n",
    "\n",
    "# Convert to pandas\n",
    "df = df_spark.toPandas()\n",
    "print(f\"Data loaded successfully! Shape: {df.shape}\")\n",
    "\n",
    "# Prepare features and labels\n",
    "feature_columns = [col for col in df.columns if col.startswith('feature_')]\n",
    "X = df[feature_columns].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=CONFIG['test_size'], \n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_subsets(n_features, n_subsets):\n",
    "    \"\"\"\n",
    "    Generate diverse feature subsets for GPU models\n",
    "    \"\"\"\n",
    "    np.random.seed(CONFIG['random_state'] + 100)  # Different seed for GPU models\n",
    "    subsets = []\n",
    "    \n",
    "    # Calculate feature importance scores\n",
    "    f_scores, _ = f_classif(X_train, y_train)\n",
    "    mi_scores = mutual_info_classif(X_train, y_train, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    top_f_features = np.argsort(f_scores)[::-1]\n",
    "    top_mi_features = np.argsort(mi_scores)[::-1]\n",
    "    \n",
    "    for i in range(n_subsets):\n",
    "        subset_size = np.random.randint(30, n_features + 1)  # 30-100 features for deep learning\n",
    "        strategy = i % 5  # Use fewer strategies for smaller model count\n",
    "        \n",
    "        if strategy == 0:\n",
    "            subset = list(range(n_features))\n",
    "        elif strategy == 1:\n",
    "            subset = sorted(np.random.choice(n_features, subset_size, replace=False))\n",
    "        elif strategy == 2:\n",
    "            subset = sorted(top_f_features[:subset_size])\n",
    "        elif strategy == 3:\n",
    "            subset = sorted(top_mi_features[:subset_size])\n",
    "        else:\n",
    "            n_top = subset_size // 2\n",
    "            top_features = list(top_f_features[:n_top])\n",
    "            remaining = [f for f in range(n_features) if f not in top_features]\n",
    "            random_features = list(np.random.choice(remaining, subset_size - n_top, replace=False))\n",
    "            subset = sorted(top_features + random_features)\n",
    "        \n",
    "        subsets.append({\n",
    "            'feature_indices': subset,\n",
    "            'n_features': len(subset),\n",
    "            'strategy': ['all', 'random', 'top_f', 'top_mi', 'f_random_mix'][strategy]\n",
    "        })\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "# Generate feature subsets\n",
    "n_features = X.shape[1]\n",
    "feature_subsets = generate_feature_subsets(n_features, CONFIG['n_models_total'])\n",
    "\n",
    "print(f\"Generated {len(feature_subsets)} feature subsets for GPU models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleMLP(nn.Module):\n",
    "    \"\"\"Flexible Multi-Layer Perceptron for binary classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, dropout=0.0):\n",
    "        super(FlexibleMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "print(\"PyTorch MLP model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space for PyTorch models\n",
    "HYPERPARAMETER_SPACE = {\n",
    "    'hidden_sizes': [(64,), (128,), (256,), (64, 32), (128, 64), (256, 128), (128, 64, 32), (256, 128, 64)],\n",
    "    'learning_rate': (1e-4, 1e-2, 'log'),\n",
    "    'batch_size': [32, 64, 128, 256],\n",
    "    'dropout': (0.0, 0.5),\n",
    "    'epochs': [50, 100],\n",
    "    'weight_decay': (1e-6, 1e-3, 'log')\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter space defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_model(hyperparams, X_tr, y_tr, X_val, y_val, device='cuda'):\n",
    "    \"\"\"Train a PyTorch MLP model\"\"\"\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_scaled = scaler.fit_transform(X_tr)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tr_tensor = torch.FloatTensor(X_tr_scaled).to(device)\n",
    "    y_tr_tensor = torch.FloatTensor(y_tr).unsqueeze(1).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_scaled).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val).unsqueeze(1).to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=hyperparams['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X_tr.shape[1]\n",
    "    model = FlexibleMLP(\n",
    "        input_size=input_size,\n",
    "        hidden_sizes=hyperparams['hidden_sizes'],\n",
    "        dropout=hyperparams['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=hyperparams['learning_rate'],\n",
    "        weight_decay=hyperparams['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(hyperparams['epochs']):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_proba = model(X_val_tensor).cpu().numpy().flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    y_val_np = y_val_tensor.cpu().numpy().flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_val_np, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_val_np, y_pred_proba),\n",
    "        'f1': f1_score(y_val_np, y_pred),\n",
    "        'precision': precision_score(y_val_np, y_pred),\n",
    "        'recall': recall_score(y_val_np, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_hyperparameters(trial):\n",
    "    \"\"\"Suggest hyperparameters using Optuna\"\"\"\n",
    "    hyperparams = {}\n",
    "    \n",
    "    hyperparams['hidden_sizes'] = trial.suggest_categorical(\n",
    "        'hidden_sizes', \n",
    "        HYPERPARAMETER_SPACE['hidden_sizes']\n",
    "    )\n",
    "    hyperparams['learning_rate'] = trial.suggest_float(\n",
    "        'learning_rate', \n",
    "        HYPERPARAMETER_SPACE['learning_rate'][0],\n",
    "        HYPERPARAMETER_SPACE['learning_rate'][1],\n",
    "        log=True\n",
    "    )\n",
    "    hyperparams['batch_size'] = trial.suggest_categorical(\n",
    "        'batch_size',\n",
    "        HYPERPARAMETER_SPACE['batch_size']\n",
    "    )\n",
    "    hyperparams['dropout'] = trial.suggest_float(\n",
    "        'dropout',\n",
    "        HYPERPARAMETER_SPACE['dropout'][0],\n",
    "        HYPERPARAMETER_SPACE['dropout'][1]\n",
    "    )\n",
    "    hyperparams['epochs'] = trial.suggest_categorical(\n",
    "        'epochs',\n",
    "        HYPERPARAMETER_SPACE['epochs']\n",
    "    )\n",
    "    hyperparams['weight_decay'] = trial.suggest_float(\n",
    "        'weight_decay',\n",
    "        HYPERPARAMETER_SPACE['weight_decay'][0],\n",
    "        HYPERPARAMETER_SPACE['weight_decay'][1],\n",
    "        log=True\n",
    "    )\n",
    "    \n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(X_tr, y_tr, X_val, y_val, n_trials=20, device='cuda'):\n",
    "    \"\"\"Optimize hyperparameters using Bayesian optimization\"\"\"\n",
    "    \n",
    "    def objective(trial):\n",
    "        hyperparams = suggest_hyperparameters(trial)\n",
    "        \n",
    "        try:\n",
    "            metrics = train_pytorch_model(hyperparams, X_tr, y_tr, X_val, y_val, device)\n",
    "            return metrics['roc_auc']\n",
    "        except Exception:\n",
    "            return 0.5\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        sampler=TPESampler(seed=CONFIG['random_state'])\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    \n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "print(\"Bayesian optimization functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ray Remote Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def train_model_with_optimization(\n",
    "    model_id,\n",
    "    feature_subset,\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    X_test_full,\n",
    "    y_test_full,\n",
    "    n_trials=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Ray remote function to train a PyTorch model on GPU\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Determine which GPU to use\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "            # Ray will handle GPU assignment\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            print(f\"Warning: CUDA not available for model {model_id}, using CPU\")\n",
    "        \n",
    "        # Extract feature subset\n",
    "        feature_indices = feature_subset['feature_indices']\n",
    "        X_train = X_train_full[:, feature_indices]\n",
    "        X_test = X_test_full[:, feature_indices]\n",
    "        \n",
    "        # Split for validation\n",
    "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "            X_train, y_train_full,\n",
    "            test_size=0.2,\n",
    "            random_state=CONFIG['random_state'],\n",
    "            stratify=y_train_full\n",
    "        )\n",
    "        \n",
    "        # Optimize hyperparameters\n",
    "        best_hyperparams, best_val_score = optimize_hyperparameters(\n",
    "            X_tr, y_tr, X_val, y_val, n_trials, device\n",
    "        )\n",
    "        \n",
    "        # Train final model on full training set\n",
    "        test_metrics = train_pytorch_model(\n",
    "            best_hyperparams, X_train, y_train_full, X_test, y_test_full, device\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        result = {\n",
    "            'model_id': model_id,\n",
    "            'model_type': 'pytorch_mlp',\n",
    "            'cluster_type': 'gpu',\n",
    "            'n_features_used': len(feature_indices),\n",
    "            'feature_strategy': feature_subset['strategy'],\n",
    "            'best_hyperparams': json.dumps({k: str(v) for k, v in best_hyperparams.items()}),\n",
    "            'best_val_score': best_val_score,\n",
    "            'accuracy': test_metrics['accuracy'],\n",
    "            'roc_auc': test_metrics['roc_auc'],\n",
    "            'f1': test_metrics['f1'],\n",
    "            'precision': test_metrics['precision'],\n",
    "            'recall': test_metrics['recall'],\n",
    "            'training_time': training_time,\n",
    "            'device': device,\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model_id': model_id,\n",
    "            'model_type': 'pytorch_mlp',\n",
    "            'cluster_type': 'gpu',\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'training_time': time.time() - start_time\n",
    "        }\n",
    "\n",
    "print(\"Ray remote training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray cluster using Databricks utilities\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster\n",
    "\n",
    "# Shutdown any existing Ray instance\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "try:\n",
    "    # For GPU cluster, setup Ray to use single-node with multiple GPUs\n",
    "    print(\"Setting up Ray cluster on GPU node...\")\n",
    "    setup_ray_cluster(\n",
    "        num_worker_nodes=1,  # Single-node GPU cluster\n",
    "        num_cpus_worker_node=16,  # Adjust based on your GPU cluster's CPU count\n",
    "        num_gpus_worker_node=CONFIG['n_gpus'],  # 4 GPUs\n",
    "        collect_log_to_path=\"/Workspace/Users/ryuta.yoshimatsu@databricks.com/ray_logs\"\n",
    "    )\n",
    "    \n",
    "    # Explicitly initialize Ray after cluster setup\n",
    "    ray.init(address='auto', ignore_reinit_error=True, logging_level='ERROR')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: setup_ray_cluster failed with: {e}\")\n",
    "    print(\"Falling back to standard Ray initialization...\")\n",
    "    # Fallback to standard initialization\n",
    "    ray.init(ignore_reinit_error=True, logging_level='ERROR')\n",
    "\n",
    "print(\"\\nRay initialized successfully on GPU cluster!\")\n",
    "print(f\"Available CPUs: {ray.cluster_resources().get('CPU', 0)}\")\n",
    "print(f\"Available GPUs: {ray.cluster_resources().get('GPU', 0)}\")\n",
    "print(f\"Expected GPUs: {CONFIG['n_gpus']}\")\n",
    "\n",
    "# Verify cluster setup\n",
    "print(f\"\\nCluster nodes connected: {len(ray.nodes())}\")\n",
    "print(f\"Total cluster resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model configurations\n",
    "model_configs = []\n",
    "model_id = CONFIG['model_id_start']\n",
    "\n",
    "for i in range(CONFIG['n_models_total']):\n",
    "    feature_subset = feature_subsets[i]\n",
    "    \n",
    "    config = {\n",
    "        'model_id': model_id,\n",
    "        'feature_subset': feature_subset\n",
    "    }\n",
    "    \n",
    "    model_configs.append(config)\n",
    "    model_id += 1\n",
    "\n",
    "print(f\"Generated {len(model_configs)} GPU model configurations\")\n",
    "print(f\"Model IDs: {CONFIG['model_id_start']} to {model_id - 1}\")\n",
    "print(f\"Each model will use 1 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Launch Parallel Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data in Ray object store\n",
    "X_train_ref = ray.put(X_train)\n",
    "y_train_ref = ray.put(y_train)\n",
    "X_test_ref = ray.put(X_test)\n",
    "y_test_ref = ray.put(y_test)\n",
    "\n",
    "print(\"Data stored in Ray object store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training jobs\n",
    "print(f\"\\nLaunching {len(model_configs)} PyTorch training jobs on GPU cluster...\\n\")\n",
    "print(\"=\"*80)\n",
    "start_time = time.time()\n",
    "\n",
    "futures = []\n",
    "for config in model_configs:\n",
    "    future = train_model_with_optimization.remote(\n",
    "        model_id=config['model_id'],\n",
    "        feature_subset=config['feature_subset'],\n",
    "        X_train_full=X_train_ref,\n",
    "        y_train_full=y_train_ref,\n",
    "        X_test_full=X_test_ref,\n",
    "        y_test_full=y_test_ref,\n",
    "        n_trials=CONFIG['n_trials_per_model']\n",
    "    )\n",
    "    \n",
    "    futures.append(future)\n",
    "\n",
    "print(f\"All {len(futures)} jobs submitted. Waiting for results...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "results = []\n",
    "completed = 0\n",
    "remaining_futures = futures.copy()\n",
    "\n",
    "while remaining_futures:\n",
    "    ready_futures, remaining_futures = ray.wait(remaining_futures, num_returns=1)\n",
    "    \n",
    "    for future in ready_futures:\n",
    "        result = ray.get(future)\n",
    "        results.append(result)\n",
    "        completed += 1\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"[{completed}/{len(futures)}] Model {result['model_id']} (PyTorch MLP) - \"\n",
    "                  f\"ROC AUC: {result['roc_auc']:.4f} ({result['training_time']:.1f}s) [{result['device'].upper()}]\")\n",
    "        else:\n",
    "            print(f\"[{completed}/{len(futures)}] Model {result['model_id']} FAILED\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GPU CLUSTER TRAINING COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "print(f\"Successful: {sum(1 for r in results if r['status'] == 'success')}\")\n",
    "print(f\"Failed: {sum(1 for r in results if r['status'] == 'failed')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Ray cluster\n",
    "shutdown_ray_cluster()\n",
    "print(\"Ray cluster shut down successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Results to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter successful results\n",
    "successful_results = [r for r in results if r['status'] == 'success']\n",
    "\n",
    "if successful_results:\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(successful_results)\n",
    "    results_df['training_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    results_spark_df = spark.createDataFrame(results_df)\n",
    "    \n",
    "    # Save to Delta table (append mode)\n",
    "    print(f\"\\nSaving {len(results_df)} results to {CONFIG['results_table']}...\")\n",
    "    results_spark_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(CONFIG['results_table'])\n",
    "    \n",
    "    print(f\"Results saved successfully!\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"  Mean ROC AUC: {results_df['roc_auc'].mean():.4f}\")\n",
    "    print(f\"  Best ROC AUC: {results_df['roc_auc'].max():.4f}\")\n",
    "    print(f\"  Mean Accuracy: {results_df['accuracy'].mean():.4f}\")\n",
    "    print(f\"  Best Accuracy: {results_df['accuracy'].max():.4f}\")\n",
    "    print(f\"  Total training time: {results_df['training_time'].sum():.2f}s\")\n",
    "    print(f\"  Average training time per model: {results_df['training_time'].mean():.2f}s\")\n",
    "else:\n",
    "    print(\"No successful results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully trained **10 PyTorch MLP models** on the GPU cluster:\n",
    "- Efficient parallel execution using Ray with GPU allocation\n",
    "- Bayesian hyperparameter optimization for each model\n",
    "- Various neural network architectures\n",
    "- Different feature selection strategies\n",
    "- Results saved to shared Delta table: `ryuta.ray.model_training_results`\n",
    "\n",
    "**Next Steps**:\n",
    "1. Use the analysis notebook to compare CPU and GPU results\n",
    "2. Identify the best performing models overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
